---
title: "Distributional Reinforcement Learning for LLMs (DistRL)"
excerpt: "用分布式价值函数 + 不确定性引导探索，系统提升 LLM 推理。"
date: 2025-09-29
layout: single
author_profile: true
categories: [Research]
tags: [Reinforcement Learning, LLMs, ICLR2026]
permalink: /posts/distrl/
header:
  overlay_image: /assets/img/distrl-fig2.png
  overlay_filter: 0.3
---

传统 actor–critic 使用**标量价值**，忽略了不确定性信息。LLM 推理环境近似**确定性**，分布的“扩展”可以作为**纯参数不确定性**信号来引导探索。

## 方法概览
- **Distributional Critic（Center–Delta Quantile Head）**：输出有序分位数，显式建模回报分布。
- **D-GAE**：稳健的多步分布式学习目标。
- **DLTV Exploration Bonus**：上半部尾部方差作为不确定性；随训练步数衰减。
- **Optimistic Advantage**：将 bonus 融合入 PPO 目标。

## 结果亮点
- 在 AIME24/25、MATH500、Minerva、OlympiadBench、College、AMC、BeyondAIME 上，DistRL 持续优于 PPO/GRPO/DAPO。
- Qwen2.5-Math-7B：Avg pass@k 由 67.3 → **71.2**；Avg avg@k 由 43.5 → **44.1**。
- BeyondAIME：`pass@1024` 由 52.0 → **58.0**。

> 代码与更多图表：Paper / Repo 链接（替换为你的）。
