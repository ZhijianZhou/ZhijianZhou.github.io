<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Distributional Reinforcement Learning for Large Language Models</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/water.css@2/out/water.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    body {
      max-width: 900px;
      margin: auto;
      padding: 20px;
    }
    h1, h2, h3 {
      text-align: center;
    }
    .authors {
      text-align: center;
      margin-bottom: 2em;
    }
    figure {
      text-align: center;
      margin: 2em auto;
    }
    figcaption {
      font-size: 0.9em;
      color: #555;
    }
    .nav-header {
      text-align: center;
      margin-bottom: 2em;
      padding-bottom: 1em;
      border-bottom: 1px solid #eee;
    }
    .nav-header a {
      display: inline-block;
      padding: 8px 16px;
      background: #007acc;
      color: white;
      text-decoration: none;
      border-radius: 5px;
      font-weight: 500;
      transition: background 0.3s ease;
    }
    .nav-header a:hover {
      background: #005fa3;
    }
  </style>
</head>
<body>

  <div class="nav-header">
    <a href="../">← Back to Research Directory</a>
  </div>

  <h1>Distributional Reinforcement Learning for Large Language Models</h1>

  <div class="authors">
    <!-- TODO: Add author list here -->
    <p><em>Author list will be inserted here</em></p>
  </div>

  <h2>Abstract</h2>
  <p>
    Actor-critic reinforcement learning for large language models (LLMs) typically relies on a scalar 
    value function, discarding crucial information about potential returns. We propose a distributional 
    actor-critic framework that learns the full distribution of returns to guide exploration more effectively. 
    ... (rest of abstract) ...
  </p>

  <h2>1 Introduction</h2>
  <p>
    Large language models (LLMs) have recently achieved remarkable progress ...
  </p>

  <h2>2 Preliminaries</h2>
  <p>
    A Markov Decision Process (MDP) is defined by the tuple \((S, A, P, R, \gamma)\) ...
  </p>

  <h2>3 Method</h2>
  <p>
    We propose a distributional actor-critic framework ... including D-GAE and DLTV bonus.
  </p>

  <h2>4 Related Work</h2>
  <p>
    Reinforcement Learning for LLMs ... Distributional RL ... Exploration in RL ...
  </p>

  <h2>5 Experiments</h2>
  <p>
    We evaluate our method on benchmarks including AIME24/25, MATH500, Minerva, OlympiadBench ...
  </p>

  <figure>
    <img src="../static/images/DistRL/distrl-fig2.png" alt="Figure 1" width="600">
    <figcaption>Figure 1: Uncertainty in general RL vs. LLM RL.</figcaption>
  </figure>

  <h2>6 Conclusion</h2>
  <p>
    We presented a distributional actor–critic framework ... achieving state-of-the-art results on 
    Qwen2.5-Math-7B and BeyondAIME benchmarks.
  </p>

  <h2>References</h2>
  <ul>
    <li>Bellemare et al. (2017). A Distributional Perspective on Reinforcement Learning. ICML.</li>
    <li>Dabney et al. (2018). Distributional Reinforcement Learning with Quantile Regression. AAAI.</li>
    <li>Schulman et al. (2017). Proximal Policy Optimization Algorithms. arXiv.</li>
    <!-- etc... -->
  </ul>

</body>
</html>
