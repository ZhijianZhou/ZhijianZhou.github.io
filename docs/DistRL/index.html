<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Distributional Reinforcement Learning for Large Language Models">
  <meta name="keywords" content="Distributional RL, LLM, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>DistRL: Distributional RL for LLMs</title>

  <link rel="shortcut icon" href="https://picx.zhimg.com/v2-cb40b1f8c3125f3cfb9a4538e1c0f2b7_l.jpg?source=32738c0c" type="image/x-icon">
  <link href="../static/css" rel="stylesheet">

  <link rel="stylesheet" href="../static/bulma.min.css">
  <link rel="stylesheet" href="../static/bulma-carousel.min.css">
  <link rel="stylesheet" href="../static/bulma-slider.min.css">
  <link rel="stylesheet" href="../static/fontawesome.all.min.css">
  <link rel="stylesheet" href="../static/academicons.min.css">
  <link rel="stylesheet" href="../static/index.css">
  <link rel="stylesheet" href="../static/leaderboard.css">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <script type="text/javascript" src="../static/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../static/fontawesome.all.min.js"></script>
  <script src="../static/bulma-carousel.min.js"></script>
  <script src="../static/bulma-slider.min.js"></script>
  <script src="../static/explorer-index.js"></script>
  <script src="../static/question_card.js"></script>

  <script src="../static/leaderboard_testmini.js"></script>

  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a class="navbar-item" href="../">
      <span class="icon">
        <i class="fas fa-arrow-left"></i>
      </span>
      <span>Back to Research Directory</span>
    </a>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <span>Distributional Reinforcement Learning for Large Language Models</span>
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#"><b>Zhijian Zhou</b></a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#"><b>Author 2</b></a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#"><b>Author 3</b></a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="margin-right: 15px;"><sup>1</sup>University/Institution 1,</span>
            <span class="author-block" style="margin-right: 15px;"><sup>2</sup>University/Institution 2</span>
          </div>

          <!-- Paper Links -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link -->
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- ArXiv Link -->
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Actor-critic reinforcement learning for large language models (LLMs) typically
            relies on a scalar value function, discarding crucial information about potential
            returns. We propose a distributional actor-critic framework that learns the full
            distribution of returns to guide exploration more effectively. We find that in deter-
            ministic reasoning tasks, the spread of this learned distribution directly measures
            the model’s confidence in its own value estimates. Our method harnesses this sig-
            nal through an optimistic exploration bonus derived from the distribution’s upper-
            tail variance, guiding the policy toward promising yet uncertain reasoning paths.
            This uncertainty-guided exploration promotes the discovery of diverse correct so-
            lutions, leading to substantial gains in pass@k across challenging benchmarks.
            This result demonstrates a significant enhancement of the model’s upper-bound
            reasoning capacity over strong baselines, which is complemented by consistent,
            albeit more modest, improvements in single-answer correctness.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method Overview -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="content has-text-centered" style="margin-top: 1cm;">
        <img src="../static/images/DistRL/distrl-fig2.png" alt="DistRL Method">
      </div>
    </div>
    
    <div class="hero-body">
      <h2 class="has-text-justified" style="margin-top: -1cm;">
        <p class=""><strong>Figure 1:</strong> Uncertainty in general RL vs. LLM RL. 
          (a) In classical RL, both parametric and intrinsic uncertainties coexist. 
          (b) In LLM RL, the environment is deterministic and intrinsic uncertainty vanishes, leaving only parametric uncertainty as the driver of exploration.</p>
      </h2>
    </div>
  </div>
</section>

<!-- Main Content -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content">
          
          <h2 class="title is-4">1. Introduction</h2>
          <p>
            Large language models (LLMs) have recently achieved remarkable progress in various reasoning tasks. 
            However, traditional reinforcement learning approaches for LLMs rely on scalar value functions, 
            potentially missing crucial distributional information that could guide more effective exploration.
            Our work addresses this limitation by proposing a distributional actor-critic framework.
          </p>

          <h2 class="title is-4">2. Preliminaries</h2>
          <p>
            A Markov Decision Process (MDP) is defined by the tuple \((S, A, P, R, \gamma)\) where \(S\) is the 
            state space, \(A\) is the action space, \(P\) is the transition probability, \(R\) is the reward 
            function, and \(\gamma\) is the discount factor. In the context of LLM fine-tuning, states correspond 
            to partial sequences, actions to token selections, and rewards to task-specific feedback.
          </p>

          <h2 class="title is-4">3. Method</h2>
          <p>
            We propose a distributional actor-critic framework that extends traditional policy gradient methods 
            by learning the full return distribution. Our approach includes two key innovations: <strong>Distributional 
            Generalized Advantage Estimation (D-GAE)</strong> and <strong>Distributional Lower-Tail Variance (DLTV) bonus</strong>. 
            These components work together to provide more informative exploration signals for LLM reasoning tasks.
          </p>

          <h2 class="title is-4">4. Experiments</h2>
          <p>
            We evaluate our method on challenging mathematical reasoning benchmarks including AIME24/25, MATH500, 
            Minerva, and OlympiadBench. Our distributional approach consistently outperforms baseline methods 
            across all tested domains, achieving state-of-the-art results on Qwen2.5-Math-7B and significant 
            improvements on BeyondAIME benchmarks.
          </p>

          <h2 class="title is-4">5. Conclusion</h2>
          <p>
            We presented a distributional actor-critic framework for LLM fine-tuning that effectively captures 
            return uncertainty to guide exploration. The proposed D-GAE and DLTV bonus mechanisms enable more 
            robust and effective learning in the complex reasoning scenarios typical of large language models.
          </p>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{distrl2024,
  title={Distributional Reinforcement Learning for Large Language Models},
  author={Zhou, Zhijian and others},
  journal={arXiv preprint},
  year={2024}
}
    </code></pre>
  </div>
</section>

</body>
</html>
