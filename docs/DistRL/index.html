<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Distributional Reinforcement Learning for Large Language Models">
  <meta name="keywords" content="Distributional RL, LLM, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>DistRL: Distributional RL for LLMs</title>

  <link rel="shortcut icon" href="https://picx.zhimg.com/v2-cb40b1f8c3125f3cfb9a4538e1c0f2b7_l.jpg?source=32738c0c" type="image/x-icon">
  <link href="../static/css" rel="stylesheet">

  <link rel="stylesheet" href="../static/bulma.min.css">
  <link rel="stylesheet" href="../static/bulma-carousel.min.css">
  <link rel="stylesheet" href="../static/bulma-slider.min.css">
  <link rel="stylesheet" href="../static/fontawesome.all.min.css">
  <link rel="stylesheet" href="../static/academicons.min.css">
  <link rel="stylesheet" href="../static/index.css">
  <link rel="stylesheet" href="../static/leaderboard.css">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <script type="text/javascript" src="../static/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../static/fontawesome.all.min.js"></script>
  <script src="../static/bulma-carousel.min.js"></script>
  <script src="../static/bulma-slider.min.js"></script>
  <script src="../static/explorer-index.js"></script>
  <script src="../static/question_card.js"></script>

  <script src="../static/leaderboard_testmini.js"></script>

  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a class="navbar-item" href="../">
      <span class="icon">
        <i class="fas fa-arrow-left"></i>
      </span>
      <span>Back to Research Directory</span>
    </a>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <span>Distributional Reinforcement Learning for Large Language Models</span>
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <b>Zhijian Zhou</b><sup>1,2,3</sup>,</span>
            <span class="author-block">
              <b>Long li</b><sup>1</sup>,</span>
            <span class="author-block">
              <b>Xuan Zhang</b><sup>1,2,3</sup>
            </span>
            <span class="author-block">
              <b>Zongkai Liu</b><sup>2,3</sup>
            </span>
            <span class="author-block">
              <b>Yanting Miao</b><sup>4</sup>
            </span>
            <span class="author-block">
              <b>Yuchen Liu</b><sup>1</sup>
            </span>
            <span class="author-block">
              <b>Deshu Chen</b><sup>1</sup>
            </span>
            <span class="author-block">
              <b>Ke Li</b><sup>3</sup>
            </span>
            <span class="author-block">
              <b>Xing Sun</b><sup>3</sup>
            </span>
            <span class="author-block">
              <b>Xiaoyu Tan</b><sup>3</sup><sup>†</sup>
            </span>
            <span class="author-block">
              <b>Chao Qu</b><sup>1</sup><sup>*</sup>
            </span>
            <span class="author-block">
              <b>Yuan Qi</b><sup>1,2</sup><sup>*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="margin-right: 15px;"><sup>1</sup>Fudan University,</span>
            <span class="author-block" style="margin-right: 15px;"><sup>2</sup>Shanghai Innovation Institute,</span>
            <span class="author-block" style="margin-right: 15px;"><sup>3</sup>Tencent Youtu Lab,</span>
            <span class="author-block" style="margin-right: 15px;"><sup>4</sup>University of Waterloo</span>
          </div>

          <!-- Paper Links -->
          <div class="column has-text-centered">
              <!-- Code Link -->
              <span class="link-block">
                <a href="https://github.com/ZhijianZhou/verl_DRL" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Actor-critic reinforcement learning for large language models (LLMs) typically
            relies on a scalar value function, discarding crucial information about potential
            returns. We propose a distributional actor-critic framework that learns the full
            distribution of returns to guide exploration more effectively. We find that in deter
            ministic reasoning tasks, the spread of this learned distribution directly measures
            the model’s confidence in its own value estimates. Our method harnesses this sig
            nal through an optimistic exploration bonus derived from the distribution’s upper-tail variance, guiding the policy toward promising yet uncertain reasoning paths.
            This uncertainty-guided exploration promotes the discovery of diverse correct so
            lutions, leading to substantial gains in pass@k across challenging benchmarks.
            This result demonstrates a significant enhancement of the model’s upper-bound
            reasoning capacity over strong baselines, which is complemented by consistent,
            albeit more modest, improvements in single-answer correctness.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method Overview -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="content has-text-centered" style="margin-top: 1cm;">
        <img src="../static/images/DistRL/distrl-fig2.png" alt="DistRL Method">
      </div>
    </div>
    
    <div class="hero-body">
      <h2 class="has-text-justified" style="margin-top: -1cm;">
        <p class=""><strong>Figure 1:</strong> Uncertainty in general RL vs. LLM RL. 
          (a) In classical RL, both parametric and intrinsic uncertainties coexist. 
          (b) In LLM RL, the environment is deterministic and intrinsic uncertainty vanishes, leaving only parametric uncertainty as the driver of exploration.</p>
      </h2>
    </div>
  </div>
</section>

<!-- Main Content -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content">
          
          <h2 class="title is-4">Introduction</h2>
            <p>
              In conventional actor–critic reinforcement learning, the critic estimates a scalar value function, 
              such as \( Q^\pi(s,a) \) or \( V^\pi(s) \), representing the expected cumulative return under policy \(\pi\). 
              While this captures average performance, it ignores higher-order characteristics of the return 
              distribution—variance, skewness, and tail behavior. Such information can be critical for exploration 
              and robust decision-making.
            </p>

            <p>
              Distributional reinforcement learning (DistRL) addresses this limitation by modeling the full 
              distribution of returns, \( Z^\pi(s,a) \), rather than only its expectation. Approaches such as 
              Quantile Regression DQN (QR-DQN) and Implicit Quantile Networks (IQN) demonstrated improved 
              performance by capturing this richer signal for learning.
            </p>

            <p>
              Crucially, the learned distribution \( Z^\pi(s,a) \) captures two types of uncertainty. 
              <em>Intrinsic uncertainty</em> stems from randomness in environment dynamics or rewards, while 
              <em>parametric uncertainty</em> reflects the model’s own estimation error due to finite data. 
              In most RL environments, these two are mixed together. However, in LLM-based reasoning, the 
              environment is effectively deterministic: the next state follows deterministically from the 
              current tokens and chosen action, and rewards such as correctness are also deterministic. 
              As a result, intrinsic uncertainty vanishes, and the spread of the learned distribution can be 
              interpreted as a pure measure of <strong>parametric uncertainty</strong>—a direct signal of the 
              critic’s confidence in its estimates.
            </p>

            <p>
              This insight allows us to repurpose the distributional critic as a quantifier of estimation 
              uncertainty for guiding exploration. While conventional PPO-based methods discard this information 
              with a scalar value head, our approach leverages it. Specifically, we introduce a 
              <strong>distributional actor–critic framework</strong> that uses the upper-tail variance of 
              \( Z^\pi(s,a) \) as an optimistic intrinsic reward. This encourages exploration of states and 
              actions where the critic is uncertain about potentially high returns, enabling more directed and 
              efficient exploration in complex reasoning tasks.
            </p>

            <h2 class="title is-4">Contributions</h2>
            <ul>
              <li>
                We reframe the role of distributional critics for LLMs, showing that deterministic 
                chain-of-thought reasoning eliminates intrinsic uncertainty. The return distribution thus 
                provides a pure estimate of parametric uncertainty.
              </li>
              <li>
                We introduce a parametric-uncertainty-guided exploration bonus, derived from the 
                upper-tail variance of the learned distribution. Combined with a tailored 
                Distributional Generalized Advantage Estimation (D-GAE), this effectively guides the policy.
              </li>
              <li>
                Across challenging reasoning benchmarks, our method consistently improves pass@<em>k</em> 
                across sampling budgets, enhancing the upper-bound reasoning capacity of LLMs. 
                Qualitative analysis further shows that our exploration strategy shifts the model from 
                superficial trial-and-error to more structured reasoning.
              </li>
            </ul>

        </div>
      </div>
    </div>
  </div>
</section>

</body>
</html>
