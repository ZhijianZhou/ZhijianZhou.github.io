<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Distributional Reinforcement Learning for Large Language Models">
  <meta name="keywords" content="Distributional RL, LLM, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>DistRL: Distributional RL for LLMs</title>

  <link rel="shortcut icon" href="https://picx.zhimg.com/v2-cb40b1f8c3125f3cfb9a4538e1c0f2b7_l.jpg?source=32738c0c" type="image/x-icon">
  <link href="../static/css" rel="stylesheet">

  <link rel="stylesheet" href="../static/bulma.min.css">
  <link rel="stylesheet" href="../static/bulma-carousel.min.css">
  <link rel="stylesheet" href="../static/bulma-slider.min.css">
  <link rel="stylesheet" href="../static/fontawesome.all.min.css">
  <link rel="stylesheet" href="../static/academicons.min.css">
  <link rel="stylesheet" href="../static/index.css">
  <link rel="stylesheet" href="../static/leaderboard.css">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <script type="text/javascript" src="../static/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../static/fontawesome.all.min.js"></script>
  <script src="../static/bulma-carousel.min.js"></script>
  <script src="../static/bulma-slider.min.js"></script>
  <script src="../static/explorer-index.js"></script>
  <script src="../static/question_card.js"></script>

  <script src="../static/leaderboard_testmini.js"></script>

  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a class="navbar-item" href="../">
      <span class="icon">
        <i class="fas fa-arrow-left"></i>
      </span>
      <span>Back to Research Directory</span>
    </a>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <span>Distributional Reinforcement Learning for Large Language Models</span>
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <b>Zhijian Zhou</b><sup>1,2,3</sup>,</span>
            <span class="author-block">
              <b>Long li</b><sup>1</sup>,</span>
            <span class="author-block">
              <b>Xuan Zhang</b><sup>1,2,3</sup>
            </span>
            <span class="author-block">
              <b>Zongkai Liu</b><sup>2,3</sup>
            </span>
            <span class="author-block">
              <b>Yanting Miao</b><sup>4</sup>
            </span>
            <span class="author-block">
              <b>Yuchen Liu</b><sup>1</sup>
            </span>
            <span class="author-block">
              <b>Deshu Chen</b><sup>1</sup>
            </span>
            <span class="author-block">
              <b>Ke Li</b><sup>3</sup>
            </span>
            <span class="author-block">
              <b>Xing Sun</b><sup>3</sup>
            </span>
            <span class="author-block">
              <b>Xiaoyu Tan</b><sup>3</sup><sup>†</sup>
            </span>
            <span class="author-block">
              <b>Chao Qu</b><sup>1</sup><sup>*</sup>
            </span>
            <span class="author-block">
              <b>Yuan Qi</b><sup>1,2</sup><sup>*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="margin-right: 15px;"><sup>1</sup>Fudan University,</span>
            <span class="author-block" style="margin-right: 15px;"><sup>2</sup>Shanghai Innovation Institute,</span>
            <span class="author-block" style="margin-right: 15px;"><sup>3</sup>Tencent Youtu Lab,</span>
            <span class="author-block" style="margin-right: 15px;"><sup>4</sup>University of Waterloo</span>
          </div>

          <!-- Paper Links -->
          <div class="column has-text-centered">
              <!-- Code Link -->
              <span class="link-block">
                <a href="https://github.com/ZhijianZhou/verl_DRL" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Actor-critic reinforcement learning for large language models (LLMs) typically
            relies on a scalar value function, discarding crucial information about potential
            returns. We propose a distributional actor-critic framework that learns the full
            distribution of returns to guide exploration more effectively. We find that in deter
            ministic reasoning tasks, the spread of this learned distribution directly measures
            the model’s confidence in its own value estimates. Our method harnesses this sig
            nal through an optimistic exploration bonus derived from the distribution’s upper-tail variance, guiding the policy toward promising yet uncertain reasoning paths.
            This uncertainty-guided exploration promotes the discovery of diverse correct so
            lutions, leading to substantial gains in pass@k across challenging benchmarks.
            This result demonstrates a significant enhancement of the model’s upper-bound
            reasoning capacity over strong baselines, which is complemented by consistent,
            albeit more modest, improvements in single-answer correctness.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method Overview -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="content has-text-centered" style="margin-top: 1cm;">
        <img src="../static/images/DistRL/distrl-fig2.png" alt="DistRL Method">
      </div>
    </div>
    
    <div class="hero-body">
      <h2 class="has-text-justified" style="margin-top: -1cm;">
        <p class=""><strong>Figure 1:</strong> Uncertainty in general RL vs. LLM RL. 
          (a) In classical RL, both parametric and intrinsic uncertainties coexist. 
          (b) In LLM RL, the environment is deterministic and intrinsic uncertainty vanishes, leaving only parametric uncertainty as the driver of exploration.</p>
      </h2>
    </div>
  </div>
</section>

<!-- Main Content -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content">
          
          <h2 class="title is-4">Introduction</h2>
            <p>
              In conventional actor–critic reinforcement learning, the critic estimates a scalar value function 
              (e.g., \( Q^\pi(s,a) \) or \( V^\pi(s) \)), which captures only the expected return and ignores 
              higher-order properties of the return distribution. Distributional reinforcement learning (DistRL) 
              addresses this by modeling the full return distribution \( Z^\pi(s,a) \), preserving information such 
              as variance and tail behavior that can provide richer signals for exploration and more robust learning.
            </p>
            
            <p>
              Crucially, the learned distribution \( Z^\pi(s,a) \) captures two types of uncertainty. 
              <em>Intrinsic uncertainty</em> stems from randomness in environment dynamics or rewards, while 
              <em>parametric uncertainty</em> reflects the model’s own estimation error due to finite data. 
              In most RL environments, these two are mixed together. However, in LLM-based reasoning, the 
              environment is effectively deterministic: the next state follows deterministically from the 
              current tokens and chosen action, and rewards such as correctness are also deterministic. 
              As a result, intrinsic uncertainty vanishes, and the spread of the learned distribution can be 
              interpreted as a pure measure of <strong>parametric uncertainty</strong>—a direct signal of the 
              critic’s confidence in its estimates (see <strong>Figure&nbsp;1</strong>).
            </p>
            
            <p>
              This insight allows us to repurpose the distributional critic as a quantifier of estimation 
              uncertainty for guiding exploration. While conventional PPO-based methods discard this information 
              with a scalar value head, our approach leverages it. Specifically, we introduce a 
              <strong>distributional actor–critic framework</strong> that uses the upper-tail variance of 
              \( Z^\pi(s,a) \) as an optimistic intrinsic reward. This encourages exploration of states and 
              actions where the critic is uncertain about potentially high returns, enabling more directed and 
              efficient exploration in complex reasoning tasks.
            </p>
          </h2>

            <h2 class="title is-4">Contributions</h2>
            <ul>
              <li>
                We reframe the role of distributional critics for LLMs, showing that deterministic 
                chain-of-thought reasoning eliminates intrinsic uncertainty. The return distribution thus 
                provides a pure estimate of parametric uncertainty.
              </li>
              <li>
                We introduce a parametric-uncertainty-guided exploration bonus, derived from the 
                upper-tail variance of the learned distribution. Combined with a tailored 
                Distributional Generalized Advantage Estimation (D-GAE), this effectively guides the policy.
              </li>
              <li>
                Across challenging reasoning benchmarks, our method consistently improves pass@<em>k</em> 
                across sampling budgets, enhancing the upper-bound reasoning capacity of LLMs. 
                Qualitative analysis further shows that our exploration strategy shifts the model from 
                superficial trial-and-error to more structured reasoning.
              </li>
            </ul>
            <!-- Method Framework Figure -->
            <div class="content has-text-centered" style="margin-top: 1.5cm; margin-bottom: 1.5cm;">
              <img src="../static/images/DistRL/method.png" alt="DistRL Framework Overview" style="max-width:90%;">
              <p style="margin-top: 0.5em;">
                <strong>Figure 2:</strong> The actor maps a prompt to a response. 
                The critic takes the prompt–response pair and, through a critic backbone 
                (architecture identical to the actor's and initialized with the same parameters), 
                produces hidden states \(h_s\) that are fed into a distributional critic head to yield a return distribution \(Q_{s_t}\). 
                Unlike PPO's scalar head \(V_t\), our head outputs an ordered set of quantiles \(\{q_{s_t}^{(n)}\}\); 
                the scalar value used in losses is the expectation of this distribution.
                These quantiles supervise critic learning via quantile Huber regression and drive actor updates through
                Distributional Generalized Advantage Estimation (D-GAE) with a Decaying Left-Truncated Variance (DLTV) exploration bonus. 
                D-GAE returns (i) the multi-step target \(\hat{y}_t\) for quantile regression and (ii) an optimistic advantage \(\tilde{A}_t\) for actor learning. 
                Black arrows indicate inference; red arrows indicate training (reward assignment, advantage estimation, critic optimization). 
                Dashed boxes detail the center–delta quantile head (top left), the truncated-variance bonus \(B_{T_p}\) (top right), 
                and the D-GAE equations (bottom right).
              </p>
            </div>

            <h2 class="title is-4">Method</h2>

              <p>
                We propose a <strong>distributional actor–critic framework</strong> designed to enhance the reasoning capabilities of large language models (LLMs).
                Our approach operates within an on-policy RL loop, akin to PPO, where trajectories are iteratively sampled and used for policy and value function updates.
                The core innovation lies in replacing the conventional scalar critic with a <em>distributional critic</em> that models the entire probability
                distribution of the state-value return, denoted by the random variable \( Z^\pi(s) \).
                This richer representation serves two synergistic purposes (see <strong>Figure&nbsp;2</strong> for an overview of our framework).
              </p>
            
              <ol style="margin-left:1.25rem;">
                <li>
                  It provides stable, multi-step <em>distributional</em> targets for robust critic training, achieved via
                  <strong>Distributional Generalized Advantage Estimation (D-GAE)</strong>.
                </li>
                <li>
                  It enables quantification of model uncertainty, which we harness for exploration through a
                  <strong>Decaying Left-Truncated Variance (DLTV)</strong> bonus.
                </li>
              </ol>

              <p>
                A key distinction from QR-DQN–style value learning is our focus on the <em>state-value</em> return \( Z(s) \) rather than the state–action return \( Z(s,a) \).
                This is crucial because (i) in LLMs the action space (the full vocabulary) is extremely large, making per-action distributions impractical; and
                (ii) in actor–critic pipelines like PPO, the critic’s primary role is to estimate \( V(s) \) for advantage computation.
                <strong>Therefore, we model the distribution of the state-value return \( Z(s) \)</strong>.
                We denote the \(i\)-th quantile of this distribution by \( q^{(i)}(s) \).
              </p>

              <p>
                Architecturally, we ensure the predicted state-value quantiles
                \( \{\,q^{(1)}(s) &lt; q^{(2)}(s) &lt; \cdots &lt; q^{(N)}(s)\,\} \)
                are strictly monotonic by design. The critic head predicts a central quantile and a sequence of non-negative deltas,
                which are cumulatively added (left/right) to form the ordered quantiles (see “Center–Delta Quantile Head” below).
              </p>

              <h3 class="title is-5">Critic Update via Distributional GAE</h3>
              <p>
                For each timestep \(t\), we compute \(N\) quantile-wise TD errors based on the state-value function:
              </p>
              <p>
                \[
                  \delta_t^{(i)} \;=\; r_t \;+\; \gamma\, q_{\theta}^{(i)}(s_{t+1}) \;-\; q_{\theta}^{(i)}(s_t).
                \]
              </p>
              <p>
                Analogous to standard GAE, we define the distributional advantages recursively:
              </p>
              <p>
                \[
                  \mathcal{A}_t^{(i)} \;=\; \delta_t^{(i)} \;+\; \gamma\lambda\,\mathcal{A}_{t+1}^{(i)}
                  \quad\Longleftrightarrow\quad
                  \mathcal{A}_t^{(i)} \;=\; \sum_{k=0}^{\infty} (\gamma\lambda)^k \,\delta_{t+k}^{(i)} .
                \]
              </p>
              <p>
                The multi-step target for each quantile is
              </p>
              <p>
                \[
                  \hat{y}_t^{(i)} \;=\; q_{\theta}^{(i)}(s_t) \;+\; \mathcal{A}_t^{(i)}.
                \]
              </p>
              <p>
                With predicted quantiles \( \{q_\theta^{(i)}(s_t)\} \) and targets \( \{\hat{y}_t^{(j)}\} \), the critic minimizes the quantile Huber loss:
              </p>
              <p>
                \[
                \mathcal{L}_{\text{critic}}(\theta)
                \;=\;
                \mathbb{E}_{\tau}\!\left[
                  \frac{1}{T}\sum_{t=0}^{T-1}
                  \frac{1}{N^2}
                  \sum_{i=1}^{N}\sum_{j=1}^{N}
                  \rho_{\tau_i}^{\kappa}\!\Big(\hat{y}_t^{(j)} - q_\theta^{(i)}(s_t)\Big)
                \right].
                \]
              </p>

              <h3 class="title is-5">Uncertainty-Guided Exploration with DLTV</h3>
              <p>
                We design a <strong>Decaying Left-Truncated Variance (DLTV)</strong> bonus to operationalize optimism in the face of uncertainty.
                The exploration bonus for state \(s\) at training step \(T_{\text{step}}\) is
              </p>
              <p>
                \[
                  \mathcal{B}_{T_{\text{step}}}(s) \;=\; \eta_{T_{\text{step}}} \cdot \sigma_{+}(s),
                \]
              </p>
              <p>
                where the <em>upper-tail</em> standard deviation is
              </p>
              <p>
                \[
                  \sigma_{+}(s)
                  \;=\;
                  \sqrt{\frac{1}{N/2}\;
                    \sum_{i=\frac{N}{2}+1}^{N}
                    \Big(q^{(i)}_\theta(s) - q^{(N/2)}_{\theta}(s)\Big)^2 }.
                \]
              </p>
              <p>
                The decay schedule gradually reduces exploration pressure:
              </p>
              <p>
                \[
                  \eta_{T_{\text{step}}}
                  \;=\;
                  c \cdot \sqrt{\frac{\log T_{\text{step}}}{\,T_{\text{step}}\,}} ,
                \]
              </p>
              <p>
                with coefficient \(c\) controlling the annealing rate.
              </p>

              <h3 class="title is-5">Actor Update with Optimistic Advantage</h3>
              <p>
                We first compute a scalar baseline value by averaging the predicted quantiles:
              </p>
              <p>
                \[
                  v_t \;=\; \frac{1}{N}\sum_{i=1}^{N} q_{\theta}^{(i)}(s_t).
                \]
              </p>
              <p>
                The standard (scalar) GAE is then
              </p>
              <p>
                \[
                  \delta_t \;=\; r_t + \gamma v_{t+1} - v_t,
                  \qquad
                  A_t \;=\; \delta_t + \gamma \lambda A_{t+1}.
                \]
              </p>
              <p>
                We obtain the <strong>optimistic advantage</strong> by augmenting \(A_t\) with the DLTV bonus on the next state and clipping for stability:
              </p>
              <p>
                \[
                  \tilde{A}_t(s_t,a_t)
                  \;=\;
                  A_t(s_t,a_t)
                  \;+\;
                  \min\!\Big(
                    \alpha \cdot \mathcal{B}_{t+1}^{\text{detach}}(s_{t+1}),
                    \;\frac{|A_t|}{\kappa}
                  \Big).
                \]
              </p>
              <p>
                Here \( \alpha \) scales the bonus, \( \kappa \) controls relative clipping, and “detach” indicates the bonus is not back-propagated through the critic.
                The optimistic advantage \( \tilde{A}_t \) is used in the PPO surrogate objective to update the policy.
              </p>

              <h3 class="title is-5">Center–Delta Quantile Head</h3>
              <p>
                To guarantee strictly ordered quantiles without post-hoc sorting, we use a <strong>center–delta</strong> head.
                Given a hidden representation \(h_t\), one branch predicts a center quantile \(c_t\), and another predicts non-negative deltas
                \( \{\Delta_k\}_{k=1}^{N-1} \) (via <code>softplus</code>). Splitting deltas to the left/right of the center index \(m=\lfloor N/2 \rfloor\) and cumulatively summing yields:
              </p>
              <p>
                \[
                  q_{m-r}(t) \;=\; c_t - \sum_{k=1}^{r}\Delta^{L}_{t,k},\quad
                  q_{m}(t) \;=\; c_t,\quad
                  q_{m+r}(t) \;=\; c_t + \sum_{k=1}^{r}\Delta^{R}_{t,k},
                \]
              </p>
              <p>
                ensuring \( q^{(1)}(t) &lt; q^{(2)}(t) &lt; \cdots &lt; q^{(N)}(t) \) by construction.
              </p>
              <h2 class="title is-4">Experiments</h2>

                <h3 class="title is-5">Experiment Setup</h3>

                <p>
                  We evaluate our method on the DAPO-Math-17k dataset of 17K curated competition-style math problems, 
                  using two 7B-parameter backbones from the Qwen2.5 series: the math-specialized Qwen2.5-Math-7B and the general-purpose Qwen2.5-7B-Base. 
                  Baselines include GRPO, DAPO, and PPO; to ensure fairness, the PPO implementation is strengthened with recent techniques such as 
                  Clip-Higher rewards, token-level loss, critic pre-training, and group sampling. 
                  Performance is assessed on AIME2024/25, MATH500, AMC, OlympiadBench, College Math, Minerva, and the challenging BeyondAIME benchmark. 
                  We report <strong>Pass@k</strong> (success if at least one of k samples is correct) and <strong>Avg@k</strong> (average correctness across k samples), 
                  following standard decoding protocols.
                </p>
        </div>
      </div>
    </div>
  </div>
</section>

</body>
</html>
